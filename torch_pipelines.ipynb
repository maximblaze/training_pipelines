{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W-y0eZL-r4xp",
        "3fY_g5GRE_UL",
        "_RngW5l0splB",
        "LQKJyv6C2RJV",
        "2yNuSMXCsJSo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Basic data reading for data in folders.\n",
        "- Returns:\n",
        "  \n",
        "  train_data_path: List[str]"
      ],
      "metadata": {
        "id": "Bwh5g21Dr_P4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting basic training configs"
      ],
      "metadata": {
        "id": "W-y0eZL-r4xp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BZvtGahqdHu"
      },
      "outputs": [],
      "source": [
        "# разные режимы датасета\n",
        "DATA_MODES = ['train', 'val', 'test']\n",
        "# все изображения будут масштабированы к размеру 224x224 px. Размер, хорошо воспринимаемый сетями, предобученными на ImageNet\n",
        "RESCALE_SIZE = 224\n",
        "# работаем на видеокарте\n",
        "DEVICE = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Torch Dataset"
      ],
      "metadata": {
        "id": "3fY_g5GRE_UL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- input: List[str]\n",
        "\n",
        "\"\"\"\n",
        "list of train urls\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6sO_gmifG6-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import torch\n",
        "from scipy.io import wavfile\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset\n",
        "import whisper\n",
        "from pydub import AudioSegment\n",
        "\n",
        "class CustomAudioDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Датасет с картинками, который паралельно подгружает их из папок\n",
        "    производит скалирование и превращение в торчевые тензоры, а также добавляет аугментации\n",
        "    \"\"\"\n",
        "    def __init__(self, mode, files, labels=None, train_transforms=None, val_test_transforms=None):\n",
        "        super().__init__()\n",
        "        # список файлов для загрузки\n",
        "        self.files = files\n",
        "        # режим работы\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode not in DATA_MODES:\n",
        "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
        "            raise NameError\n",
        "\n",
        "        self.len_ = len(self.files)\n",
        "\n",
        "        self.train_transforms = train_transforms\n",
        "        self.val_test_transforms = val_test_transforms\n",
        "\n",
        "        if self.mode != 'test':\n",
        "            self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_\n",
        "\n",
        "    def load_audio_sample(self, file):\n",
        "\n",
        "#        audio_bytes = file.read()\n",
        "#        wav_readed = wavfile.read(io.BytesIO(audio_bytes))[1]\n",
        "#        audio = torch.from_numpy(wav_readed)\n",
        "\n",
        "        # sound = AudioSegment.from_mp3(file)   #если данные в формате mp3 переведем их в wav\n",
        "        # file = sound.export(format=\"wav\")\n",
        "\n",
        "        # audio = torchaudio.load(file)[0]  #for other models may be better\n",
        "\n",
        "        audio = whisper.audio.load_audio(file)  #only for whisper - special format\n",
        "        return audio\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # введем тут наши аугментации для train и val/test данных.\n",
        "        x = self.load_audio_sample(self.files[index])\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            if self.train_transforms:\n",
        "                transform = self.train_transforms\n",
        "                x = transform(x)\n",
        "        else:\n",
        "            if self.val_test_transforms:\n",
        "                transform = self.val_test_transforms\n",
        "                x = transform(x)\n",
        "        if self.mode == 'test':\n",
        "            return x\n",
        "        else:\n",
        "            label = self.labels[index]\n",
        "            y = label\n",
        "            return x, y"
      ],
      "metadata": {
        "id": "cx7qZ5UErzr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wavfile reading:\n",
        "import io\n",
        "import torch\n",
        "\n",
        "def wav_read(wavfile):\n",
        "  audio_bytes = wavfile.read()\n",
        "  wav_readed = wavfile.read(io.BytesIO(audio_bytes))[1]\n",
        "  audio = torch.from_numpy(wav_readed)\n",
        "  return audio"
      ],
      "metadata": {
        "id": "19E81k6vUpJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fleur dataset as tarfile\n",
        "\n",
        "import tarfile\n",
        "from scipy.io import wavfile\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def download(url: str, target_path: str):\n",
        "    with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n",
        "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
        "            while True:\n",
        "                buffer = source.read(8192)\n",
        "                if not buffer:\n",
        "                    break\n",
        "\n",
        "                output.write(buffer)\n",
        "                loop.update(len(buffer))\n",
        "\n",
        "\n",
        "class Fleurs(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, files, labels):\n",
        "        url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n",
        "        tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n",
        "        os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n",
        "        if not os.path.exists(tar_path):\n",
        "            download(url, tar_path)\n",
        "\n",
        "        all_audio = []\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            for member in tar.getmembers():\n",
        "                name = member.name\n",
        "                audio_bytes = tar.extractfile(member).read()\n",
        "                all_audio.append(wavfile.read(io.BytesIO(audio_bytes))[1])\n",
        "\n",
        "        self.labels = labels\n",
        "        self.all_audio = all_audio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        record_label = self.labels[index]\n",
        "        audio = torch.from_numpy(self.all_audio[index].copy())\n",
        "        #text = record_label[\"transcription\"]\n",
        "\n",
        "        return (audio, record_label)"
      ],
      "metadata": {
        "id": "OwhJWCGttyw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Classification Torch DataSet configuration"
      ],
      "metadata": {
        "id": "_RngW5l0splB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Датасет с картинками, который паралельно подгружает их из папок\n",
        "    производит скалирование и превращение в торчевые тензоры, а также добавляет аугментации\n",
        "    \"\"\"\n",
        "    def __init__(self, files, labels, label_encoder_path, train_transforms,\n",
        "                 val_test_transforms, mode):\n",
        "        super().__init__()\n",
        "        # список файлов для загрузки\n",
        "        self.files = files\n",
        "        # режим работы\n",
        "        self.mode = mode\n",
        "        self.train_transforms = train_transforms\n",
        "        self.val_test_transforms = val_test_transforms\n",
        "\n",
        "        if self.mode not in DATA_MODES:\n",
        "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
        "            raise NameError\n",
        "\n",
        "        self.len_ = len(self.files)\n",
        "\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        if self.mode != 'test':\n",
        "            self.labels = labels\n",
        "            self.label_encoder.fit(self.labels)\n",
        "\n",
        "            with open(label_encoder_path, 'wb') as le_dump_file:\n",
        "                  pickle.dump(self.label_encoder, le_dump_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len_\n",
        "\n",
        "    def load_sample(self, file):\n",
        "        image = Image.open(file)\n",
        "        image.load()\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # введем тут наши аугментации для train и val/test данных.\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            transform = self.train_transforms\n",
        "        else:\n",
        "            transform = self.val_test_transforms\n",
        "\n",
        "        x = self.load_sample(self.files[index])\n",
        "        x = transform(x)\n",
        "        if self.mode == 'test':\n",
        "            return x\n",
        "        else:\n",
        "            label = self.labels[index]\n",
        "            label_id = self.label_encoder.transform([label])\n",
        "            y = label_id.item()\n",
        "            return x, y"
      ],
      "metadata": {
        "id": "VvPEPW66sAPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),  #аугментация переворотов по горизонтали\n",
        "                transforms.RandomVerticalFlip(p=0.5), #аугментация переворотов по вертикали\n",
        "                #transforms.Pad(padding = 15, padding_mode = 'constant'),\n",
        "                transforms.ToTensor(),\n",
        "\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])"
      ],
      "metadata": {
        "id": "3MOCp_57tONt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_test_transforms = transforms.Compose([\n",
        "                transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])"
      ],
      "metadata": {
        "id": "hllU_WOMtqLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image classifier model loader"
      ],
      "metadata": {
        "id": "LQKJyv6C2RJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "efnet_v2_model = torch.hub.load('hankyul2/EfficientNetV2-pytorch', 'efficientnet_v2_m', pretrained=True)\n",
        "#кастомный классификатор\n",
        "class Custom_Classifier_efnet_v2(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(Custom_Classifier_efnet_v2, self).__init__()\n",
        "        self.model = model\n",
        "        self.classifier = nn.Linear(1000, 20)  #numver\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = F.selu(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "Custom_Classifier_efnet_v2_model = Custom_Classifier_efnet_v2(efnet_v2_model)\n",
        "final_model = Custom_Classifier_efnet_v2_model\n",
        "final_model.to(DEVICE)"
      ],
      "metadata": {
        "id": "OM95hl492T_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loops in native Torch"
      ],
      "metadata": {
        "id": "2yNuSMXCsJSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#back propagation fit step\n",
        "def fit_epoch(model, train_loader, criterion, optimizer):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_data = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        processed_data += inputs.size(0)\n",
        "\n",
        "    train_loss = running_loss / processed_data\n",
        "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "7ZONMqRT04WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Шаг прямого распространения\n",
        "from scipy.stats import mode\n",
        "def eval_epoch(model, val_loader, criterion, min_loss, eps, model_name):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    processed_size = 0\n",
        "    running_incorrects = []\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        all_preds.extend(list(preds.cpu()))\n",
        "        all_labels.extend(list(labels.data.cpu()))\n",
        "        for i in range(len(preds)):\n",
        "            if preds[i] != labels.data[i]:\n",
        "                running_incorrects.append(labels.data[i].cpu())\n",
        "        processed_size += inputs.size(0)\n",
        "    f1 = f1_score(all_labels, all_preds, average = 'weighted')\n",
        "    print(f'f1 weighted = {f1}')\n",
        "    most_frequent = mode(list(running_incorrects))[0][0]\n",
        "    #этот блок нужен для того, чтобы сохранять только самую лучшую модель по лоссу на валидации\n",
        "    val_loss = running_loss / processed_size\n",
        "    if val_loss < min_loss or val_loss == min_loss+eps:\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "    val_acc = running_corrects.double() / processed_size\n",
        "    return val_loss, val_acc, most_frequent"
      ],
      "metadata": {
        "id": "n0JRj2Af16gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция, собирающая всё вместе для обучения и сохраняющая лог\n",
        "model_weights_path = 'yolov5/classification_5_col_marked/model_classifier_mono_corrected_weights/'\n",
        "#os.mkdir(model_weights_path)\n",
        "def train(train_files, val_files, model, epochs, batch_size, weights_for_class, model_name):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2, shuffle=False)\n",
        "\n",
        "    history = []\n",
        "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
        "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
        "\n",
        "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
        "        opt = torch.optim.AdamW(model.parameters(), lr=0.0001)#, weight_decay=0.005)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(opt, 1, 0.5) #введём scheduler чтобы уменьшать learning rate динамически во время обучения\n",
        "        criterion = nn.CrossEntropyLoss()#weight=weights_for_class.to(DEVICE))\n",
        "        min_loss = np.inf\n",
        "        eps = 0.001\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
        "            print(\"loss\", train_loss)\n",
        "\n",
        "            val_loss, val_acc, most_frequent = eval_epoch(model, val_loader, criterion, min_loss, eps, model_name)\n",
        "            print(f'самая частая ошибка в классе - {most_frequent}')\n",
        "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
        "            scheduler.step()\n",
        "            pbar_outer.update(1)\n",
        "            torch.save(model.state_dict(), model_weights_path + model_name + f'__epoch - {epoch+1}'+'.pt')  #Будем всё равно сохранять веса после каждой эпохи\n",
        "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
        "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
        "    return history"
      ],
      "metadata": {
        "id": "LrF3OVzm2BH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция предикта\n",
        "def predict(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        logits = []\n",
        "        for inputs in test_loader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            model.eval()\n",
        "            outputs = model(inputs).cpu()\n",
        "            logits.append(outputs)\n",
        "\n",
        "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
        "    return probs"
      ],
      "metadata": {
        "id": "tFpoNOhR2Ike"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'custom_efnet_V2_model_weighted_20_class_corrected_monoculture'\n",
        "weights_for_class = [] #no weighted cross entropy\n",
        "num_epochs = 15\n",
        "batch_size = 32\n",
        "\n",
        "history = train(train_dataset,\n",
        "                val_dataset,\n",
        "                model=final_model,\n",
        "                epochs=num_epochs,\n",
        "                batch_size=batch_size,\n",
        "                weights_for_class=weights_for_class,\n",
        "                model_name=model_name)"
      ],
      "metadata": {
        "id": "GLH4UuBn2LG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.load_state_dict(torch.load(model_weights_path+f\"{model_name}__epoch - 6.pt\"))"
      ],
      "metadata": {
        "id": "TLx0drx_2e7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = pickle.load(open(label_encoder_class, 'rb'))"
      ],
      "metadata": {
        "id": "fNdKcbsh2iBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = GermsDataset(test_files, labels=None, mode=\"test\")\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=32)\n",
        "\n",
        "probs_1 = predict(final_model, test_loader)\n",
        "preds_1 = label_encoder.inverse_transform(np.argmax(probs_1, axis=1))"
      ],
      "metadata": {
        "id": "xPdCCJ9a2k0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "print(f'test accuracy = {accuracy_score(preds_1, test_labels)}')\n",
        "print(f'test F1_macro = {f1_score(preds_1, test_labels, average=\"macro\")}')\n",
        "print(f'test F1_weighted = {f1_score(preds_1, test_labels, average=\"weighted\")}')"
      ],
      "metadata": {
        "id": "KSYr6J-r2oQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_labels, preds_1))"
      ],
      "metadata": {
        "id": "O77Ng-8t2lVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}