{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nfrom matplotlib import pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, confusion_matrix, \\\nmultilabel_confusion_matrix, classification_report\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring Multi-label text classification with Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"## What is Multi-label classification\n\nThere are mainly tree types of classification problems:\n    \n    Bynary classification puts the samples in only one of two possible classes. A person can be either a male or a female.\n\n    Multiclass classification means a classification task with more than two classes; e.g., classify a set of people in different age groups. Multiclass classification makes the assumption that each sample is assigned to one and only one label: a person can only be between 20 and 29 years old or 30 to 39 years old but not in both groups.\n    \n    Multilabel classification assigns to each sample a set of target labels. This can be thought as predicting properties of a data-point that are not mutually exclusive, such as topics that are relevant for a document. A text might be about any of religion, politics, finance or education at the same time or none of these.\n\nThere are two main methods for tackling a multi-label classification problem: problem transformation methods and algorithm adaptation methods.\nProblem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers. They include OneVsRest and Binary Relevance techniques.\n\nWhereas algorithm adaptation methods adapt the algorithms to directly perform multi-label classification. In other words, rather than trying to convert the problem to a simpler problem, they try to address the problem in its full form. They include Classifier Chains, Label Powerset and Adapted Algorithm."},{"metadata":{},"cell_type":"markdown","source":"## Our dataset\n\nWe will use the datasets from a Kaggle competition which aims to resolve the problem with negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). The dataset contains a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are: **toxic, severe_toxic,obscene, threat, insult,identity_hate**. A model which predicts a probability of each type of toxicity for each comment must be created.\n\nhttps://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview\n\nFirst we load our files:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest_set = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\ntest_labels = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('train set', train_set.head(10))\nprint('test set', test_set.head(10))\nprint('test labels', test_labels.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning and analisys\n\nWe need to check for empty values just in case:"},{"metadata":{"trusted":false},"cell_type":"code","source":"print(train_set.isna().sum())\nprint(test_set.isna().sum())\nprint(test_labels.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it was a competition, to deter hand labeling, the test_set contains some comments which are not included \nin scoring. We need to remove the comments containing labels with -1 from the later supplied test_labels file and the test_set."},{"metadata":{"trusted":false},"cell_type":"code","source":"test_set = test_set[test_labels['toxic'] != -1]\ntest_labels = test_labels[test_labels['toxic'] != -1]\ntest_features = test_set.comment_text\ntrain_features = train_set.comment_text\nprint('test labels', test_labels.head(10))\ntrain_labels = train_set.drop(['id', 'comment_text'], axis = 1)\ntest_labels = test_labels.drop(['id'], axis = 1)\nlabels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to try to clean the text of the comments by removing punctuation and spcial characters, converting all to lowercase, stripping white space, and replacing some commonly used short forms with their full words. We want save the cleaned comments in separate variables and test the vectorizer algorithm with both sets. We will also use the stop_words parameter of the vectorizer algorithm to remove the words that do not bring value and will actually confuse and slow down the vectorizer."},{"metadata":{"trusted":false},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text\n\ntest_features_cleaned = test_features.map(lambda com : clean_text(com))\ntrain_features_cleaned = train_features.map(lambda com : clean_text(com))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We check the percentage of labeled and non-labeled comments"},{"metadata":{"trusted":false},"cell_type":"code","source":"print('Percentage of comments without labels: ')\nprint(len((train_set[(train_set.toxic == 0) & (train_set.severe_toxic == 0) & (train_set.obscene == 0) & (train_set.insult == 0) & (train_set.insult == 0) & (train_set.identity_hate == 0)])) / len(train_set)*100)\nprint('Percentage of comments with one or more labels: ')\nprint(len(train_set[(train_set.toxic == 1) | (train_set.severe_toxic == 1) | (train_set.obscene == 1) | (train_set.insult == 1) | (train_set.insult == 1) | (train_set.identity_hate == 1)]) / len(train_set)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_labels.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count the number of comments per label"},{"metadata":{"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(8,6))\nax = fig.add_axes([0,0,1,1])\ntotal_count = []\nfor label in labels:\n    total_count.append(len(train_labels[train_labels[label] == 1]))\nax.bar(labels,total_count, color=['red', 'green', 'blue', 'purple', 'orange', 'yellow'])\nfor i,data in enumerate(total_count):\n    plt.text(i-.25, \n              data/total_count[i]+100, \n              total_count[i], \n              fontsize=12)\nplt.title('Number of comments per label')\nplt.xlabel('Labels')\nplt.ylabel('Number of comments')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For our purpose we are going to use OneVsRest classifier as the others are more complex and would take a considerable amount of time. OneVsRest classifier basically takes a classifier algorithm as a parameter and uses it to do a binary classification for each label in the dataset. We will also use pipelines to streamline the process by adding all estimators we need to be executed one after anoter. As the content we are going to analise is text we need first convert it into numeric values in order for our model to be able to process it. This is done by vectorizer algorithms that use different methods for comparing and weighing the relation between words in a single document and their relations in the whole corpus. They first collect all the unique words in a document(comment in our case) and build a dictionary of them and then vectorise each one giving it a specific weigth based on how much a word relates to a specific label within the whole corpus(the collection of all comments in our dataset). We are going to use TfidfVectorizer which is one of the widely used vectorisers."},{"metadata":{},"cell_type":"markdown","source":"## Creating baseline models\n\nIn order to choose our best classifier and improve it further we are going to test tree different classifiers - **NaiveBayes, LogisticRegression and SVM with linear kernel** using their default parameters. \n    \nThe Ðµvaluation metrics we are going to use to judge our models :\n    \n    As specified in the competition requirements we need to use AUC. \n    \n    As suggested in some other articles we will also use micro-averaging for all labels of precision, recall and F1-score\n    \nLinearSVM algorithm does not have a predict_proba method which makes it impossible to use roc_auc_score for this model. I tried with SVC(kernel='linear',probability=True) which should make it possible but it took forever to compute so I just had to abandon it. We will just use acuracy_score and micro_averages F1-score."},{"metadata":{"trusted":false},"cell_type":"code","source":"NB_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),\n                       ('nb_model', OneVsRestClassifier(MultinomialNB(), n_jobs=-1))])\n\nLR_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),\n                       ('lr_model', OneVsRestClassifier(LogisticRegression(), n_jobs=-1))])\n\n# SVM_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),\n#                        ('svm_model', OneVsRestClassifier(SVC(kernel='linear',probability=True)))])\n\nSVM_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')),\n                       ('svm_model', OneVsRestClassifier(LinearSVC(), n_jobs=-1))])\n\ndef plot_roc_curve(test_features, predict_prob):\n    fpr, tpr, thresholds = roc_curve(test_features, predict_prob)\n    plt.plot(fpr, tpr)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.title('ROC curve for toxic comments')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.grid(True)\n    plt.legend(labels)\n\ndef run_pipeline(pipeline, train_feats, train_lbls, test_feats, test_lbls):\n    pipeline.fit(train_feats, train_labels)\n    predictions = pipeline.predict(test_feats)\n    pred_proba = pipeline.predict_proba(test_feats)\n    print('roc_auc: ', roc_auc_score(test_lbls, pred_proba))\n    print('accuracy: ', accuracy_score(test_lbls, predictions))\n    print('confusion matrices: ')\n    print(multilabel_confusion_matrix(test_lbls, predictions))\n    print('classification_report: ')\n    print(classification_report(test_lbls, predictions, target_names=labels))\n    \ndef run_SVM_pipeline(pipeline, train_feats, train_lbls, test_feats, test_lbls):\n    pipeline.fit(train_feats, train_labels)\n    predictions = pipeline.predict(test_feats)\n    print('accuracy: ', accuracy_score(test_lbls, predictions))\n    print('confusion matrices: ')\n    print(multilabel_confusion_matrix(test_lbls, predictions))\n    print('classification_report: ')\n    print(classification_report(test_lbls, predictions, target_names=labels))\n    \ndef plot_pipeline_roc_curve(pipeline, train_feats, train_lbls, test_feats, test_lbls):\n    for label in labels:\n        pipeline.fit(train_feats, train_set[label])\n        pred_proba = pipeline.predict_proba(test_feats)[:,1]\n        plot_roc_curve(test_lbls[label], pred_proba)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_pipeline(NB_pipeline, train_features, train_labels, test_features, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_pipeline(LR_pipeline, train_features, train_labels, test_features, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# run_pipeline(SVM_pipeline, train_features, train_labels, test_features, test_labels)\nrun_SVM_pipeline(SVM_pipeline, train_features, train_labels, test_features, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot ROC curve for LogisticRegression model"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_pipeline_roc_curve(LR_pipeline, train_features, train_labels, test_features, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot ROC curve for NaiveBayes model"},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_pipeline_roc_curve(NB_pipeline, train_features, train_labels, test_features, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter tunning\n\nWe select the LogisticRegression classifier as our best and will try to improve our results with the following techniques:\n    improve the tf-idf vectorizer's parameters\n    improve the tf-idf vectorizer by cleaning the text of the comments\n    improve the logistic regression's parameters\n    \nWe checked the distribution of samples in our dataset earlier and we saw that it was highly unbalanced - 90% features with no labels and 10% features with one or more labels.\nIn this situation in case of bynary or multiclass classification we would be able to use GridSearch with StratifiedKFold cross validation splits to test a set of different parameters for every algorithm in our pipeline. In our case though, the StratifiedKFold does not suppport multi-label and using just KFold would not yeld any useful data splits. I decided to give it a try anyway to prove this or burst it. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# TAKES ABOUT 40min to run\n# alpha = [0.1,1,10]\n# penalty=['l1','l2']\n# n_gram=[(1,1),(1,2)]\n# param_grid = {\n#     'tfidf__ngram_range': n_gram,\n#     'lr_model__estimator__C': alpha\n# }\n# gsearch_cv = GridSearchCV(LR_pipeline, param_grid=param_grid, cv=5)\n# gsearch_cv.fit(train_features, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gsearch_cv.best_score_\n# 0.919784923325667","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gsearch_cv.best_params_\n# {'lr_model__estimator__C': 10, 'tfidf__ngram_range': (1, 2)}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"LR_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n                       ('lr_model', OneVsRestClassifier(LogisticRegression(C=10), n_jobs=-1))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_pipeline(LR_pipeline, train_features, train_labels, test_features, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Running the pipeline with the suggested best parameters actually resulted in getting a better AUC accuracy. My asumption is that it was just by chance as there is no way to do a proper training on such an inbalanced set without stratifying the folds. We will proceed with a manual tunning of the models' parameters anyway to make a comparison."},{"metadata":{},"cell_type":"markdown","source":"**We try the cleaned comments with the default parameters used for our baseline best model and see almost no difference, a slight decrease in fact with 0.0001:**\n\nwith cleaned comments and C=1, ngram_range=(1,1) - roc_auc:  0.9750820030803234\nmicro avg       0.69      0.59      0.63     14498\n\n**Second thing we try to increase n-gram range of the tf-idf. n-gram parameter defines the number of consecutive words to be combined when creating the vectorising the text content. (1,1) is the default value which means only single words will be used**\n\nwith not cleaned comments and C=1, ngram_range=(1,2) - roc_auc:  0.9734608599958284 \nmicro avg       0.68      0.59      0.63     14498\n\nwith not cleaned comments and C=1, ngram_range=(2,2) - roc_auc:  0.8695484960212974\nmicro avg       0.86      0.08      0.15     14498\n\nwith not cleaned comments and C=1, ngram_range=(1,3) - roc_auc:  0.9721529511622968\nmicro avg       0.66      0.61      0.63     14498\n\n**different values for LogisticRegression parameters\nstart with C wich is the regularization strength parameter - smaller values specify stronger regularization.**\n\nwith not cleaned comments and C=.5, ngram_range=(1,1) - roc_auc:  0.9739737234377395\nmicro avg       0.73      0.53      0.61     14498\n\nwith not cleaned comments and C=2, ngram_range=(1,1) - roc_auc:  0.9752981961370573\nmicro avg       0.67      0.63      0.64     14498\n\nwith not cleaned comments and C=3, ngram_range=(1,1) - roc_auc:  0.9749554220049567\nmicro avg       0.65      0.64      0.65     14498\n\nwith not cleaned comments and C=2.1, ngram_range=(1,1) - roc_auc:  0.9752756613723531\nmicro avg       0.66      0.63      0.65     14498\n\nwith not cleaned comments and C=1.9, ngram_range=(1,1) - roc_auc:  0.9753144853229717\nmicro avg       0.67      0.62      0.64     14498\n\n**OUR BEST PARAMS**\n**with cleaned comments and C=1.5, ngram_range=(1,1) - roc_auc:  0.9753254734889681\nmicro avg       0.67      0.62      0.64     14498**\n\nwith not cleaned comments and C=1.4, ngram_range=(1,1) - roc_auc:  0.9753073320518185\nmicro avg       0.67      0.62      0.64     14498\n\n**We select C=1.5 to be the best value and then we lastly try the penalty parameter of 'l1' as the default is 'l2'**\n\nwith not cleaned comments and C=1.5, l1, ngram_range=(1,1) - roc_auc:  0.9725936377263187\nmicro avg       0.67      0.62      0.64     14498"},{"metadata":{"trusted":false},"cell_type":"code","source":"# change parameters manually to get the results above\nLR_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n                       ('lr_model', OneVsRestClassifier(LogisticRegression(C=10), n_jobs=-1))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_pipeline(LR_pipeline, train_features_cleaned, train_labels, test_features_cleaned, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nUsing relatively simple methods and algorithms we managed to create a model with an accuracy above 97% which is quite high.\nWe could not 100% prove that cross-validation is not applicable for multi-label classification problems. Probably we need to run GridSearchCV few more times to see if we will get always tha same results.\nUsing the created setup we can continue trying to improve our model by:\n\n1. Performing different types of stemming of the words\n2. Try other algorithms for vectorizing comments' content like word2vec\n3. Try using character n-grams instead of word n-grams\n4. Try using ensemble classification algorithms"},{"metadata":{},"cell_type":"markdown","source":"## RESOURCES\n\n1. https://en.wikipedia.org/wiki/Multi-label_classification\n\n2. https://scikit-learn.org/stable/modules/multiclass.html\n\n3. https://medium.com/@saugata.paul1010/a-detailed-case-study-on-multi-label-classification-with-machine-learning-algorithms-and-72031742c9aa\n\n4. https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5\n\n5. https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}